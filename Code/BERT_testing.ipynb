{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d7832e9",
      "metadata": {
        "id": "2d7832e9"
      },
      "outputs": [],
      "source": [
        "!pip install -q bert-extractive-summarizer\n",
        "!pip install -q spacy==2.1.3\n",
        "!pip install -q transformers==2.2.2\n",
        "!pip install -q neuralcoref\n",
        "!pip install torch==1.6.0+cpu torchvision==0.7.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install bert-extractive-summarizer==0.5.0\n",
        "!pip install torch\n",
        "!pip install rouge/requirements.txt\n",
        "!pip install rouge-score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "import statistics \n",
        "import torch\n",
        "from summarizer import Summarizer\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "SQJdm8NzRCxv"
      },
      "id": "SQJdm8NzRCxv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run only if using Google Colab \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "NEn_qpKpQgFV"
      },
      "id": "NEn_qpKpQgFV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80cff2d1",
      "metadata": {
        "id": "80cff2d1"
      },
      "outputs": [],
      "source": [
        "# load in WikiHow dataset - change the filename for processing of the 512 < x < 1024 and 1024 < x < 2048 datasets \n",
        "df_results = pd.read_csv('gdrive/MyDrive/SNLP_Coursework/WikiHow_sample_leq512_results_BERT.csv')\n",
        "\n",
        "# obtain all the texts to be summarised \n",
        "myText = df_results['text'][:]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add in columns to the dataset to record the extra variables\n",
        "df_results['train_length'] = 'empty'\n",
        "df_results['summary'] = 'empty'\n",
        "df_results['summary_length'] = 'empty'\n",
        "df_results.head()"
      ],
      "metadata": {
        "id": "B7bfUCHwkLoX"
      },
      "id": "B7bfUCHwkLoX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ebe0ed0",
      "metadata": {
        "id": "5ebe0ed0"
      },
      "outputs": [],
      "source": [
        "# list to store the text for all passages \n",
        "allText = []\n",
        "\n",
        "for i, text in enumerate(myText):\n",
        "    allText.append(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a888d594",
      "metadata": {
        "id": "a888d594"
      },
      "outputs": [],
      "source": [
        "# this function removes all newline characters and returns the word length of processed input text \n",
        "def preprocess(text):\n",
        "    preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
        "    return len(preprocess_text.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a1c96c4",
      "metadata": {
        "id": "3a1c96c4"
      },
      "outputs": [],
      "source": [
        "# building the model \n",
        "model = Summarizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "381c68a4",
      "metadata": {
        "id": "381c68a4"
      },
      "outputs": [],
      "source": [
        "# initialising the list of summaries \n",
        "allSummaries = []\n",
        "\n",
        "for i, text in enumerate(allText):\n",
        "\n",
        "  # preprocess input text\n",
        "  train_length = preprocess(text)\n",
        "\n",
        "  # find the max_length of the summary (1/3 of the passage length)\n",
        "  m = round(0.33 * train_length)\n",
        "\n",
        "  # output the summary and join each word via spaces\n",
        "  summary = ''.join(model(text, max_length=m))\n",
        "\n",
        "  # add the summary to the list of summaries \n",
        "  allSummaries.append(summary)\n",
        "\n",
        "  # add the results to the dataframe, including the summary and its length in words \n",
        "  df_results['train_length'][i] = train_length\n",
        "  df_results['summary'][i] = summary\n",
        "  df_results['summary_length'][i] = len(summary.split())\n",
        "\n",
        "  if i % 100 == 0: \n",
        "    # save the dataframe to the file every 100 iterations \n",
        "    df_results.to_csv('gdrive/MyDrive/SNLP_Coursework/WikiHow_sample_leq512_results_BERT.csv')\n",
        "\n",
        "  print('Processed', i, 'out of', len(allText))\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here we calculate the rouge scores for these summaries \n",
        "df_cropped = df_results\n",
        "\n",
        "# we collected 36200 summaries out of the entire <512 dataset due to GPU limitations \n",
        "# change accordingly for the number of summaries collected for other text length ranges \n",
        "start = 36200 \n",
        "n = len(df_results)\n",
        "\n",
        "# deleting the values where the summaries were not collected \n",
        "df_cropped.drop(df_cropped.index[range(start, n)], axis=0, inplace=True)\n",
        "\n",
        "# initialising lists for rouge scores \n",
        "rouge_1_precision = []\n",
        "rouge_1_recall = []\n",
        "rouge_1_fmeasure = []\n",
        "\n",
        "rouge_2_precision = []\n",
        "rouge_2_recall = []\n",
        "rouge_2_fmeasure = []\n",
        "\n",
        "rouge_L_precision = []\n",
        "rouge_L_recall = []\n",
        "rouge_L_fmeasure = []\n",
        "\n",
        "rouge_Lsum_precision = []\n",
        "rouge_Lsum_recall = []\n",
        "rouge_Lsum_fmeasure = []\n",
        "\n",
        "# calculating the rouge score for each summary\n",
        "for i in range(len(df_cropped)):\n",
        "  summary = df_cropped['summary'][i]\n",
        "  headline = df_cropped['headline'][i]\n",
        "  scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)\n",
        "  \n",
        "  # ignoring cases where no summary has been generated \n",
        "  if isinstance(summary, str):\n",
        "    scores = scorer.score(summary, headline)\n",
        "\n",
        "    rouge_1_precision.append(scores['rouge1'].precision) \n",
        "    rouge_1_recall.append(scores['rouge1'].recall)\n",
        "    rouge_1_fmeasure.append(scores['rouge1'].fmeasure)\n",
        "\n",
        "    rouge_2_precision.append(scores['rouge2'].precision) \n",
        "    rouge_2_recall.append(scores['rouge2'].recall)\n",
        "    rouge_2_fmeasure.append(scores['rouge2'].fmeasure)\n",
        "\n",
        "    rouge_L_precision.append(scores['rougeL'].precision) \n",
        "    rouge_L_recall.append(scores['rougeL'].recall)\n",
        "    rouge_L_fmeasure.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "    rouge_Lsum_precision.append(scores['rougeLsum'].precision) \n",
        "    rouge_Lsum_recall.append(scores['rougeLsum'].recall)\n",
        "    rouge_Lsum_fmeasure.append(scores['rougeLsum'].fmeasure)\n",
        "\n",
        "    if i % 1000 == 0:\n",
        "      print('Processed', i, 'out of', len(df_cropped))\n",
        "\n",
        "# calculate the averages for each rouge metric \n",
        "data = {'Rouge1': [statistics.mean(rouge_1_precision), statistics.mean(rouge_1_recall), statistics.mean(rouge_1_fmeasure)], \n",
        "        'Rouge2': [statistics.mean(rouge_2_precision), statistics.mean(rouge_2_recall), statistics.mean(rouge_2_fmeasure)], \n",
        "        'RougeL': [statistics.mean(rouge_L_precision), statistics.mean(rouge_L_recall), statistics.mean(rouge_L_fmeasure)],\n",
        "        'RougeLsum': [statistics.mean(rouge_Lsum_precision), statistics.mean(rouge_Lsum_recall), statistics.mean(rouge_Lsum_fmeasure)]} \n",
        "\n",
        "# save these averages to a dataframe \n",
        "averagesBERT = pd.DataFrame(data, index=['Precision', 'Recall', 'FMeasure'])\n",
        "\n"
      ],
      "metadata": {
        "id": "5RoJt0XFmgKH"
      },
      "id": "5RoJt0XFmgKH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save averages to a file \n",
        "df_cropped.to_csv('gdrive/MyDrive/SNLP_Coursework/croppedData_WikiHow_sample_leq512_results_BERT.csv')\n",
        "averagesBERT.to_csv('gdrive/MyDrive/SNLP_Coursework/averages_WikiHow_sample_leq512_results_BERT.csv')"
      ],
      "metadata": {
        "id": "GA6ohoGo9cgT"
      },
      "id": "GA6ohoGo9cgT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating the ROUGE scores to calculate the correlation \n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)\n",
        "for i, (headline, summary) in enumerate(zip(df_cropped['headline'],df_cropped['summary'])):\n",
        "\n",
        "    # if any summaries have not been generated, skip them\n",
        "    if isinstance(summary, str) == False:\n",
        "      continue\n",
        "    scores = scorer.score(summary, headline)\n",
        "\n",
        "    # allocate the ROUGE scores to the corresponding dataframe row \n",
        "    df_cropped.loc[i,'Rouge1'] = scores['rouge1'].fmeasure\n",
        "    df_cropped.loc[i,'Rouge2'] = scores['rouge2'].fmeasure\n",
        "    df_cropped.loc[i,'RougeL'] = scores['rougeL'].fmeasure\n",
        "    df_cropped.loc[i,'RougeLsum'] = scores['rougeLsum'].fmeasure\n"
      ],
      "metadata": {
        "id": "zB-wOBjdCaje"
      },
      "id": "zB-wOBjdCaje",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain text_lengths to plot correlation \n",
        "df_cropped['text_length'].corr(df_cropped['Rouge1'])\n",
        "corr_table = {'Rouge1': [df_cropped['text_length'].corr(df_cropped['Rouge1'])], \n",
        "        'Rouge2': [df_cropped['text_length'].corr(df_cropped['Rouge2'])], \n",
        "        'RougeL': [df_cropped['text_length'].corr(df_cropped['RougeL'])],\n",
        "        'RougeLsum': [df_cropped['text_length'].corr(df_cropped['RougeLsum'])]} \n",
        "correlation = pd.DataFrame(corr_table, index=['text_length'])"
      ],
      "metadata": {
        "id": "6jN6IKvFCbq_"
      },
      "id": "6jN6IKvFCbq_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print correlation values \n",
        "correlation"
      ],
      "metadata": {
        "id": "MKCzrgfUCeCR"
      },
      "id": "MKCzrgfUCeCR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(df_cropped['text_length'],df_cropped['Rouge1'],'o')"
      ],
      "metadata": {
        "id": "wBUc4u5ECh6N"
      },
      "id": "wBUc4u5ECh6N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4K6AgDA2TG0s"
      },
      "id": "4K6AgDA2TG0s",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "BERT_testing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}